#!/bin/env python

#PBS -l nodes=1:ppn=1
#PBS -l vmem=1gb
#PBS -l walltime=00:05:00
#PBS -N archive
#PBS -q normal
#PBS -A TG-DBS170009

import json
import subprocess
import errno
import os
import sys

with open("config.json") as config_json:
    config = json.load(config_json)
    product = {}
    for dataset in config["datasets"]:
        dest=os.environ["BRAINLIFE_ARCHIVE"]+"/"+dataset["project"]
        try: 
            os.makedirs(dest)
        except OSError as exc:
            if exc.errno == errno.EEXIST and os.path.isdir(dest):
                pass
            else:
                raise

        #store .brainlife.json
        with open(dataset["dir"]+"/.brainlife.json", "w") as bljson:
            json.dump(dataset["dataset"], bljson)

        tarname=dest+"/"+dataset["dataset"]["_id"]+".tar"
        cmd=["tar", "hcvf", tarname, "-C", dataset["dir"]]

        if "files" in dataset and dataset["files"] != None:
            #old archiver had "files["dataset"]" listing all files to archive
            files=[".brainlife.json"]
            for file in dataset["files"]:
                if "filename" in file:
                    path = file["filename"]
                else:
                    path = file["dirname"]

                if "files_override" in dataset and dataset["files_override"] != None:
                    if file["id"] in dataset["files_override"]:
                        src = dataset["files_override"][file["id"]]
                        os.symlink(src, dataset["dir"]+"/"+path)
                        print("files_override:", src, path)

                if os.path.exists(dataset["dir"]+"/"+path):
                    if path == ".":
                        #if path is ".", let's expand to the list of files so that we can avoid having "." as root 
                        files.extend(os.listdir(dataset["dir"]))
                    else:
                        files.append(path)
                elif file["required"] == True:
                    print("required file is missing:"+dataset["dir"]+"/"+path)
                    sys.exit(1)

            #dedupe the files
            files_dedupe = []
            for f in files:
              if f not in files_dedupe:
                files_dedupe.append(f)
            files = files_dedupe

        else:
            #grab everything under "dir"
            files=os.listdir(dataset["dir"])

        #archive!
        cmd.extend(files)
        print(cmd)
        subprocess.call(cmd)

        #get file size 
        product[dataset["dataset"]["_id"]] = {"size":os.path.getsize(tarname)}

    with open('product.json', 'w') as productjson:
        json.dump(product, productjson)

    print("all done")
